{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oQVtDpX9jMaN"
   },
   "source": [
    "# Automatic Ticket Assignment - Capstone Project - Approach1\n",
    "\n",
    "## Problem Statement - \n",
    "\n",
    "In most of the IT organizations, the assignment of incidents to appropriate IT groups is still a manual process. Manual assignment of incidents is time consuming and requires human efforts. There may be mistakes due to human errors and resource consumption is carried out ineffectively because of the misaddressing. On the other hand, manual assignment increases the response and resolution times which result in user satisfaction deterioration / poor customer service. \n",
    "\n",
    "_<font color=blue>This capstone project intends to reduce the manual intervention of IT operations or Service desk teams by automating the ticket assignment process.The goal here is to create a text classification based ML model that can automatically  classify any new tickets by analysing ticket description to one of the relevant Assignment groups, which could be later integrated to any ITSM tool like Service Now. Based on the ticket description our model will output the probability of assigning it to one of the 74 Groups.</font>_\n",
    "\n",
    "The solution would be implemented using below approach:\n",
    "- Approach 1 - Using a traditional machine learning algorithm we would be classifying the tickets into one of the groups having more than 100 tickets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WJpHvcvhjMaR"
   },
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVB7bQ4ijMaT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Assignment_group</th>\n",
       "      <th>Target</th>\n",
       "      <th>cleaned_description</th>\n",
       "      <th>num_wds</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>uniq_wds</th>\n",
       "      <th>token_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GRP_0</td>\n",
       "      <td>L1/L2</td>\n",
       "      <td>login issue verified user detailsemployee mana...</td>\n",
       "      <td>20</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>17</td>\n",
       "      <td>login issue verified user detailsemployee mana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GRP_0</td>\n",
       "      <td>L1/L2</td>\n",
       "      <td>outlook hmjdrvpbkomuaywn teammy meetingsskype ...</td>\n",
       "      <td>12</td>\n",
       "      <td>8.083333</td>\n",
       "      <td>11</td>\n",
       "      <td>outlook hmjdrvpbkomuaywn teammy meetingsskype ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GRP_0</td>\n",
       "      <td>L1/L2</td>\n",
       "      <td>cant log vpn eylqgodmybqkwiami cannot log vpn</td>\n",
       "      <td>7</td>\n",
       "      <td>5.571429</td>\n",
       "      <td>5</td>\n",
       "      <td>cant log vpn eylqgodmybqkwiami cannot log vpn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GRP_0</td>\n",
       "      <td>L1/L2</td>\n",
       "      <td>unable access hrtool page unable access hrtool...</td>\n",
       "      <td>8</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>4</td>\n",
       "      <td>unable access hrtool page unable access hrtool...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GRP_0</td>\n",
       "      <td>L1/L2</td>\n",
       "      <td>skype error skype error</td>\n",
       "      <td>4</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>skype error skype error</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Assignment_group Target                                cleaned_description  \\\n",
       "0            GRP_0  L1/L2  login issue verified user detailsemployee mana...   \n",
       "1            GRP_0  L1/L2  outlook hmjdrvpbkomuaywn teammy meetingsskype ...   \n",
       "2            GRP_0  L1/L2      cant log vpn eylqgodmybqkwiami cannot log vpn   \n",
       "3            GRP_0  L1/L2  unable access hrtool page unable access hrtool...   \n",
       "4            GRP_0  L1/L2                            skype error skype error   \n",
       "\n",
       "   num_wds  avg_word  uniq_wds  \\\n",
       "0       20  6.900000        17   \n",
       "1       12  8.083333        11   \n",
       "2        7  5.571429         5   \n",
       "3        8  5.500000         4   \n",
       "4        4  5.000000         2   \n",
       "\n",
       "                                          token_desc  \n",
       "0  login issue verified user detailsemployee mana...  \n",
       "1  outlook hmjdrvpbkomuaywn teammy meetingsskype ...  \n",
       "2      cant log vpn eylqgodmybqkwiami cannot log vpn  \n",
       "3  unable access hrtool page unable access hrtool...  \n",
       "4                            skype error skype error  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tranlated_inc = pd.read_csv('dataset/processed_file_modellling.csv',encoding='utf-8')\n",
    "df_tranlated_inc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inc_sample = df_tranlated_inc[df_tranlated_inc['Assignment_group'].map(df_tranlated_inc['Assignment_group'].value_counts()) > 100]\n",
    "x = df_inc_sample['token_desc']\n",
    "y = df_inc_sample['Assignment_group']\n",
    "\n",
    "from sklearn import preprocessing\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "# encoding train labels \n",
    "encoder.fit(y)\n",
    "y = encoder.transform(y)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state=13,stratify=y)\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "class_weights\n",
    "\n",
    "w_array = np.ones(y_train.shape[0], dtype = 'float')\n",
    "for i, val in enumerate(y_train):\n",
    "    w_array[i] = class_weights[val]\n",
    "    \n",
    "log_cols=[\"Classifier\", \"accuracy\",\"f1_score\"]\n",
    "log = pd.DataFrame(columns=log_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes classifier for multinomial models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6588486140724946\n",
      "f1 score 0.7755462017592144\n",
      "logloss: 1.328 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      1.00      0.81       794\n",
      "           1       0.00      0.00      0.00        28\n",
      "           2       0.82      0.18      0.30        50\n",
      "           3       0.00      0.00      0.00        29\n",
      "           4       0.00      0.00      0.00        23\n",
      "           5       0.00      0.00      0.00        44\n",
      "           6       0.75      0.06      0.11        49\n",
      "           7       1.00      0.14      0.24        58\n",
      "           8       0.00      0.00      0.00        23\n",
      "           9       0.00      0.00      0.00        40\n",
      "          10       0.00      0.00      0.00        21\n",
      "          11       0.00      0.00      0.00        31\n",
      "          12       0.00      0.00      0.00        37\n",
      "          13       0.53      0.87      0.65       129\n",
      "          14       1.00      0.02      0.04        51\n",
      "\n",
      "    accuracy                           0.66      1407\n",
      "   macro avg       0.32      0.15      0.14      1407\n",
      "weighted avg       0.56      0.66      0.54      1407\n",
      "\n",
      "[[794   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 14   0   0   0   0   0   0   0   0   0   0   0   0  14   0]\n",
      " [ 40   0   9   0   0   0   0   0   0   0   0   0   0   1   0]\n",
      " [ 28   0   0   0   0   0   0   0   0   0   0   0   0   1   0]\n",
      " [ 22   0   1   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 44   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 46   0   0   0   0   0   3   0   0   0   0   0   0   0   0]\n",
      " [ 50   0   0   0   0   0   0   8   0   0   0   0   0   0   0]\n",
      " [ 23   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 39   0   0   0   0   0   1   0   0   0   0   0   0   0   0]\n",
      " [ 21   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  3   0   0   0   0   0   0   0   0   0   0   0   0  28   0]\n",
      " [ 15   0   0   0   0   0   0   0   0   0   0   0   0  22   0]\n",
      " [ 17   0   0   0   0   0   0   0   0   0   0   0   0 112   0]\n",
      " [ 14   0   1   0   0   0   0   0   0   0   0   0   0  35   1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "predictions = nb.predict_proba(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) \n",
    "print('f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_test,predictions))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "log_entry = pd.DataFrame([[\"MultinomialNB\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols)\n",
    "log = log.append(log_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear support vector machine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 OneVsRestClassifier(estimator=LinearSVC(C=1.0,\n",
       "                                                         class_weight='balanced',\n",
       "                                                         dual=True,\n",
       "                                                         fit_intercept=True,\n",
       "                                                         intercept_scaling=1,\n",
       "                                                         loss='hinge',\n",
       "                                                         max_iter=1000,\n",
       "                                                         multi_class='ovr',\n",
       "                                                         penalty='l2',\n",
       "                                                         random_state=42,\n",
       "                                                         tol=0.0001,\n",
       "                                                         verbose=0),\n",
       "                                     n_jobs=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svc = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', OneVsRestClassifier(LinearSVC(loss='hinge',random_state=42,class_weight='balanced'))),\n",
    "               ])\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7974413646055437\n",
      "f1 score 0.7971002817664733\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89       794\n",
      "           1       0.80      0.71      0.75        28\n",
      "           2       0.52      0.70      0.60        50\n",
      "           3       0.78      0.62      0.69        29\n",
      "           4       0.43      0.39      0.41        23\n",
      "           5       0.36      0.36      0.36        44\n",
      "           6       0.54      0.61      0.57        49\n",
      "           7       0.87      0.79      0.83        58\n",
      "           8       0.61      0.61      0.61        23\n",
      "           9       0.45      0.47      0.46        40\n",
      "          10       0.64      0.43      0.51        21\n",
      "          11       0.82      0.58      0.68        31\n",
      "          12       0.81      0.78      0.79        37\n",
      "          13       0.80      0.89      0.84       129\n",
      "          14       0.93      0.73      0.81        51\n",
      "\n",
      "    accuracy                           0.80      1407\n",
      "   macro avg       0.68      0.64      0.65      1407\n",
      "weighted avg       0.80      0.80      0.80      1407\n",
      "\n",
      "[[707   1  19   1   5  17  17   2   6  13   1   0   2   1   2]\n",
      " [  3  20   0   2   0   0   0   0   0   0   0   0   0   3   0]\n",
      " [  2   0  35   0   5   0   1   1   1   1   2   0   0   2   0]\n",
      " [  5   3   0  18   0   0   1   0   0   0   0   0   2   0   0]\n",
      " [ 10   0   3   0   9   0   1   0   0   0   0   0   0   0   0]\n",
      " [ 15   0   1   0   0  16   2   2   1   6   0   0   0   0   1]\n",
      " [ 18   0   1   0   0   0  30   0   0   0   0   0   0   0   0]\n",
      " [  7   0   1   0   0   1   0  46   0   2   1   0   0   0   0]\n",
      " [  4   1   0   0   0   0   2   1  14   0   1   0   0   0   0]\n",
      " [ 12   0   0   0   0   8   1   0   0  19   0   0   0   0   0]\n",
      " [  5   0   1   0   0   3   0   1   0   1   9   1   0   0   0]\n",
      " [  2   0   0   0   1   0   0   0   0   0   0  18   0  10   0]\n",
      " [  1   0   0   2   0   0   0   0   0   0   0   0  29   5   0]\n",
      " [  3   0   5   0   1   0   0   0   0   0   0   3   2 115   0]\n",
      " [  2   0   1   0   0   0   1   0   1   0   0   0   1   8  37]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = svc.predict(X_test)\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "#print (focal_loss(alpha=.25, gamma=2))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "log_entry = pd.DataFrame([[\"LinearSVC\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols)\n",
    "log = log.append(log_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=Non...\n",
       "                 SGDClassifier(alpha=0.001, average=False,\n",
       "                               class_weight='balanced', early_stopping=False,\n",
       "                               epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "                               l1_ratio=0.15, learning_rate='optimal',\n",
       "                               loss='hinge', max_iter=100, n_iter_no_change=5,\n",
       "                               n_jobs=None, penalty='l2', power_t=0.5,\n",
       "                               random_state=42, shuffle=True, tol=None,\n",
       "                               validation_fraction=0.1, verbose=0,\n",
       "                               warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=100, tol=None,class_weight='balanced')),\n",
    "               ])\n",
    "sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.728500355366027\n",
      "f1 score 0.7088474757055361\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.77      0.85       794\n",
      "           1       0.67      0.86      0.75        28\n",
      "           2       0.45      0.66      0.53        50\n",
      "           3       0.37      0.72      0.49        29\n",
      "           4       0.38      0.74      0.50        23\n",
      "           5       0.34      0.34      0.34        44\n",
      "           6       0.48      0.61      0.54        49\n",
      "           7       0.65      0.74      0.69        58\n",
      "           8       0.40      0.74      0.52        23\n",
      "           9       0.42      0.50      0.45        40\n",
      "          10       0.27      0.62      0.38        21\n",
      "          11       0.62      0.68      0.65        31\n",
      "          12       0.59      0.78      0.67        37\n",
      "          13       0.94      0.73      0.82       129\n",
      "          14       0.74      0.73      0.73        51\n",
      "\n",
      "    accuracy                           0.73      1407\n",
      "   macro avg       0.55      0.68      0.59      1407\n",
      "weighted avg       0.79      0.73      0.75      1407\n",
      "\n",
      "[[611   4  28  19  12  21  26   9  18  16  21   0   5   0   4]\n",
      " [  0  24   0   4   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  1   0  33   1   7   0   1   2   1   1   2   0   0   1   0]\n",
      " [  1   3   0  21   1   0   1   0   0   0   0   0   2   0   0]\n",
      " [  3   0   1   0  17   0   1   1   0   0   0   0   0   0   0]\n",
      " [  8   0   1   0   2  15   1   5   2   6   3   0   0   0   1]\n",
      " [ 17   0   2   0   0   0  30   0   0   0   0   0   0   0   0]\n",
      " [  1   0   1   0   0   1   0  43   3   3   6   0   0   0   0]\n",
      " [  0   0   0   3   0   0   1   2  17   0   0   0   0   0   0]\n",
      " [  8   0   1   0   0   4   1   2   0  20   2   0   1   1   0]\n",
      " [  1   0   1   0   1   2   0   2   0   1  13   0   0   0   0]\n",
      " [  0   1   0   0   1   0   0   0   0   0   1  21   2   3   2]\n",
      " [  0   0   0   4   0   0   0   0   0   0   0   1  29   1   2]\n",
      " [  0   3   5   1   4   1   0   0   1   0   0  10   6  94   4]\n",
      " [  0   1   1   4   0   0   0   0   1   1   0   2   4   0  37]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = sgd.predict(X_test)\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "log_entry = pd.DataFrame([[\"SGDClassifier\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols)\n",
    "log = log.append(log_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=Non...\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=100000.0, class_weight='balanced',\n",
       "                                    dual=False, fit_intercept=True,\n",
       "                                    intercept_scaling=1, l1_ratio=None,\n",
       "                                    max_iter=100, multi_class='auto', n_jobs=1,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg_1 = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5,class_weight='balanced')),\n",
    "               ])\n",
    "logreg_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7882018479033405\n",
      "f1 score 0.7923411421901295\n",
      "logloss: 2.231 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88       794\n",
      "           1       0.85      0.61      0.71        28\n",
      "           2       0.56      0.62      0.59        50\n",
      "           3       0.73      0.66      0.69        29\n",
      "           4       0.50      0.48      0.49        23\n",
      "           5       0.33      0.30      0.31        44\n",
      "           6       0.56      0.57      0.57        49\n",
      "           7       0.81      0.83      0.82        58\n",
      "           8       0.68      0.57      0.62        23\n",
      "           9       0.47      0.38      0.42        40\n",
      "          10       0.45      0.43      0.44        21\n",
      "          11       0.76      0.52      0.62        31\n",
      "          12       0.72      0.78      0.75        37\n",
      "          13       0.84      0.84      0.84       129\n",
      "          14       0.92      0.71      0.80        51\n",
      "\n",
      "    accuracy                           0.79      1407\n",
      "   macro avg       0.67      0.61      0.64      1407\n",
      "weighted avg       0.78      0.79      0.78      1407\n",
      "\n",
      "[[715   0  13   1   5  19  16   3   4  12   4   0   1   1   0]\n",
      " [  6  17   0   3   0   0   0   0   0   0   0   0   0   2   0]\n",
      " [  5   0  31   0   4   0   1   1   1   1   2   0   0   4   0]\n",
      " [  7   2   0  19   0   0   1   0   0   0   0   0   0   0   0]\n",
      " [ 11   0   1   0  11   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 22   0   1   0   0  13   1   3   0   3   1   0   0   0   0]\n",
      " [ 19   0   2   0   0   0  28   0   0   0   0   0   0   0   0]\n",
      " [  6   0   1   0   0   1   0  48   0   0   2   0   0   0   0]\n",
      " [  6   0   0   0   0   0   1   2  13   0   1   0   0   0   0]\n",
      " [ 18   0   0   0   0   5   1   0   0  15   1   0   0   0   0]\n",
      " [  7   0   1   0   0   1   0   2   0   1   9   0   0   0   0]\n",
      " [  3   0   0   0   0   0   0   0   0   0   0  16   3   8   1]\n",
      " [  3   0   0   2   0   0   0   0   0   0   0   0  29   2   1]\n",
      " [  2   1   4   1   2   0   0   0   0   0   0   5   4 109   1]\n",
      " [  6   0   1   0   0   0   1   0   1   0   0   0   3   3  36]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = logreg_1.predict(X_test)\n",
    "predictions = logreg_1.predict_proba(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_test,predictions))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "log_entry = pd.DataFrame([[\"LogisticRegression\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols)\n",
    "log = log.append(log_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=Non...\n",
       "                               interaction_constraints=None, learning_rate=0.1,\n",
       "                               max_delta_step=0, max_depth=7,\n",
       "                               min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints=None, n_estimators=200,\n",
       "                               n_jobs=10, nthread=10, num_parallel_tree=1,\n",
       "                               objective='multi:softprob', random_state=0,\n",
       "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
       "                               subsample=0.8, tree_method=None,\n",
       "                               validate_parameters=False, verbosity=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgboost = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)),\n",
    "               ])\n",
    "xgboost.fit(X_train, y_train,clf__sample_weight=w_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.681592039800995\n",
      "f1 score 0.6613709885634036\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.79      0.84       794\n",
      "           1       0.18      0.64      0.29        28\n",
      "           2       0.36      0.62      0.45        50\n",
      "           3       0.57      0.59      0.58        29\n",
      "           4       0.38      0.52      0.44        23\n",
      "           5       0.30      0.55      0.39        44\n",
      "           6       0.43      0.47      0.45        49\n",
      "           7       0.70      0.78      0.74        58\n",
      "           8       0.64      0.61      0.62        23\n",
      "           9       0.34      0.50      0.41        40\n",
      "          10       0.44      0.38      0.41        21\n",
      "          11       0.56      0.29      0.38        31\n",
      "          12       0.75      0.32      0.45        37\n",
      "          13       0.82      0.60      0.69       129\n",
      "          14       0.57      0.41      0.48        51\n",
      "\n",
      "    accuracy                           0.68      1407\n",
      "   macro avg       0.53      0.54      0.51      1407\n",
      "weighted avg       0.75      0.68      0.70      1407\n",
      "\n",
      "[[628   4  30   6  10  40  20   9   6  26   5   1   3   2   4]\n",
      " [  2  18   0   2   1   0   2   0   0   1   0   0   0   2   0]\n",
      " [  2   0  31   0   6   1   3   2   1   0   2   0   0   1   1]\n",
      " [  4   3   0  17   0   0   1   0   1   1   0   0   1   0   1]\n",
      " [  6   0   5   0  12   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 10   0   3   0   0  24   1   0   0   4   1   0   0   0   1]\n",
      " [ 23   0   2   0   0   0  23   1   0   0   0   0   0   0   0]\n",
      " [  2   0   3   0   0   3   1  45   0   2   2   0   0   0   0]\n",
      " [  2   0   1   1   0   1   1   3  14   0   0   0   0   0   0]\n",
      " [ 11   0   1   0   0   6   1   1   0  20   0   0   0   0   0]\n",
      " [  3   0   1   0   0   3   0   3   0   3   8   0   0   0   0]\n",
      " [  1  14   0   0   1   0   0   0   0   0   0   9   0   4   2]\n",
      " [  2  14   1   2   0   0   0   0   0   1   0   1  12   3   1]\n",
      " [  3  28   8   0   2   1   0   0   0   0   0   4   0  77   6]\n",
      " [  2  17   1   2   0   1   1   0   0   0   0   1   0   5  21]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = xgboost.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "log_entry = pd.DataFrame([[\"Xgboost\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols)\n",
    "log = log.append(log_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tune GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:   41.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  0.7943364733949457\n",
      "Best Params:  {'clf__estimator__C': 1, 'clf__estimator__loss': 'hinge'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\"clf__estimator__C\": [0.1, 1, 10, 100, 1000],  \n",
    "              'clf__estimator__loss': ['hinge','squared_hinge'],}  \n",
    "  \n",
    "clf_svc = GridSearchCV(svc, param_grid=params, refit = True, verbose = 1,scoring='f1_weighted') \n",
    "# fitting the model for grid search \n",
    "clf_svc.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Score: \", clf_svc.best_score_)\n",
    "print(\"Best Params: \", clf_svc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7974413646055437\n",
      "f1 score 0.7971002817664733\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89       794\n",
      "           1       0.80      0.71      0.75        28\n",
      "           2       0.52      0.70      0.60        50\n",
      "           3       0.78      0.62      0.69        29\n",
      "           4       0.43      0.39      0.41        23\n",
      "           5       0.36      0.36      0.36        44\n",
      "           6       0.54      0.61      0.57        49\n",
      "           7       0.87      0.79      0.83        58\n",
      "           8       0.61      0.61      0.61        23\n",
      "           9       0.45      0.47      0.46        40\n",
      "          10       0.64      0.43      0.51        21\n",
      "          11       0.82      0.58      0.68        31\n",
      "          12       0.81      0.78      0.79        37\n",
      "          13       0.80      0.89      0.84       129\n",
      "          14       0.93      0.73      0.81        51\n",
      "\n",
      "    accuracy                           0.80      1407\n",
      "   macro avg       0.68      0.64      0.65      1407\n",
      "weighted avg       0.80      0.80      0.80      1407\n",
      "\n",
      "[[707   1  19   1   5  17  17   2   6  13   1   0   2   1   2]\n",
      " [  3  20   0   2   0   0   0   0   0   0   0   0   0   3   0]\n",
      " [  2   0  35   0   5   0   1   1   1   1   2   0   0   2   0]\n",
      " [  5   3   0  18   0   0   1   0   0   0   0   0   2   0   0]\n",
      " [ 10   0   3   0   9   0   1   0   0   0   0   0   0   0   0]\n",
      " [ 15   0   1   0   0  16   2   2   1   6   0   0   0   0   1]\n",
      " [ 18   0   1   0   0   0  30   0   0   0   0   0   0   0   0]\n",
      " [  7   0   1   0   0   1   0  46   0   2   1   0   0   0   0]\n",
      " [  4   1   0   0   0   0   2   1  14   0   1   0   0   0   0]\n",
      " [ 12   0   0   0   0   8   1   0   0  19   0   0   0   0   0]\n",
      " [  5   0   1   0   0   3   0   1   0   1   9   1   0   0   0]\n",
      " [  2   0   0   0   1   0   0   0   0   0   0  18   0  10   0]\n",
      " [  1   0   0   2   0   0   0   0   0   0   0   0  29   5   0]\n",
      " [  3   0   5   0   1   0   0   0   0   0   0   3   2 115   0]\n",
      " [  2   0   1   0   0   0   1   0   1   0   0   0   1   8  37]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_svc.best_estimator_.predict(X_test)\n",
    "#predictions = clf_svc.best_estimator_.predict_proba(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "#print (\"logloss: %0.3f \" % multiclass_logloss(y_test,predictions))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "log_entry = pd.DataFrame([[\"LinearSVC_best_estimator_gcv\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols)\n",
    "log = log.append(log_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  0.7909050333862092\n",
      "Best Params:  {'clf__alpha': 0.0001, 'clf__loss': 'modified_huber', 'clf__penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"clf__loss\" : [\"hinge\", \"log\", \"squared_hinge\", \"modified_huber\"],\n",
    "    \"clf__alpha\" : [0.0001, 0.001, 0.01, 0.1],\n",
    "    \"clf__penalty\" : [\"l2\", \"l1\", \"none\"],\n",
    "}\n",
    "\n",
    "clf_sgd = GridSearchCV(sgd, param_grid=params,refit = True, verbose = 1,scoring='f1_weighted')\n",
    "clf_sgd.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Score: \", clf_sgd.best_score_)\n",
    "print(\"Best Params: \", clf_sgd.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7924662402274343\n",
      "f1 score 0.7874501781073971\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.87      0.89       794\n",
      "           1       0.76      0.79      0.77        28\n",
      "           2       0.51      0.66      0.57        50\n",
      "           3       0.70      0.66      0.68        29\n",
      "           4       0.48      0.61      0.54        23\n",
      "           5       0.36      0.36      0.36        44\n",
      "           6       0.49      0.63      0.55        49\n",
      "           7       0.83      0.84      0.84        58\n",
      "           8       0.58      0.61      0.60        23\n",
      "           9       0.48      0.55      0.51        40\n",
      "          10       0.45      0.48      0.47        21\n",
      "          11       0.57      0.55      0.56        31\n",
      "          12       0.75      0.81      0.78        37\n",
      "          13       0.89      0.84      0.86       129\n",
      "          14       0.87      0.76      0.81        51\n",
      "\n",
      "    accuracy                           0.79      1407\n",
      "   macro avg       0.64      0.67      0.65      1407\n",
      "weighted avg       0.81      0.79      0.80      1407\n",
      "\n",
      "[[691   0  19   4   5  18  22   4   6  14   5   1   2   1   2]\n",
      " [  3  22   0   1   0   0   0   0   1   0   0   0   0   1   0]\n",
      " [  2   0  33   1   6   0   1   1   1   1   2   0   0   2   0]\n",
      " [  2   4   0  19   1   0   1   0   0   0   0   0   2   0   0]\n",
      " [  5   0   2   0  14   0   1   0   1   0   0   0   0   0   0]\n",
      " [ 13   0   1   0   0  16   2   3   1   6   1   0   0   0   1]\n",
      " [ 16   0   2   0   0   0  31   0   0   0   0   0   0   0   0]\n",
      " [  4   0   1   0   0   1   0  49   0   1   2   0   0   0   0]\n",
      " [  3   1   0   0   0   0   2   1  14   1   1   0   0   0   0]\n",
      " [ 10   0   0   0   0   6   1   0   0  22   1   0   0   0   0]\n",
      " [  5   0   1   0   0   3   0   1   0   1  10   0   0   0   0]\n",
      " [  2   0   0   0   1   0   0   0   0   0   0  17   1   9   1]\n",
      " [  1   0   0   2   0   0   0   0   0   0   0   2  30   1   1]\n",
      " [  2   1   4   0   2   1   0   0   0   0   0   7   3 108   1]\n",
      " [  2   1   2   0   0   0   2   0   0   0   0   3   2   0  39]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_sgd.best_estimator_.predict(X_test)\n",
    "#predictions = clf_svc.best_estimator_.predict_proba(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "#print (\"logloss: %0.3f \" % multiclass_logloss(y_test,predictions))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "log_entry = pd.DataFrame([[\"SGD_best_estimator_gcv\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols)\n",
    "log = log.append(log_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed: 15.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  0.7833517320435001\n",
      "Best Params:  {'clf__C': 100.0, 'clf__max_iter': 4000, 'clf__penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "  'clf__penalty': ['l2'],\n",
    "  'clf__C': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0,1e2,1e4,1e5],\n",
    "  'clf__max_iter': [100,4000,5000],\n",
    "}\n",
    "\n",
    "clf_lr = GridSearchCV(logreg_1, param_grid=params,refit = True,verbose = 1,scoring='f1_weighted')\n",
    "clf_lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Score: \", clf_lr.best_score_)\n",
    "print(\"Best Params: \", clf_lr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7882018479033405\n",
      "f1 score 0.7843872551896391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88       794\n",
      "           1       0.81      0.75      0.78        28\n",
      "           2       0.53      0.62      0.57        50\n",
      "           3       0.73      0.66      0.69        29\n",
      "           4       0.42      0.48      0.45        23\n",
      "           5       0.31      0.34      0.33        44\n",
      "           6       0.53      0.65      0.59        49\n",
      "           7       0.84      0.83      0.83        58\n",
      "           8       0.59      0.70      0.64        23\n",
      "           9       0.42      0.47      0.45        40\n",
      "          10       0.45      0.48      0.47        21\n",
      "          11       0.68      0.55      0.61        31\n",
      "          12       0.73      0.81      0.77        37\n",
      "          13       0.87      0.84      0.85       129\n",
      "          14       0.91      0.78      0.84        51\n",
      "\n",
      "    accuracy                           0.79      1407\n",
      "   macro avg       0.65      0.66      0.65      1407\n",
      "weighted avg       0.80      0.79      0.79      1407\n",
      "\n",
      "[[691   0  16   2   5  24  21   2   7  17   5   0   2   1   1]\n",
      " [  3  21   0   2   0   0   0   0   0   0   0   0   0   2   0]\n",
      " [  4   0  31   0   7   0   1   1   1   1   2   0   0   2   0]\n",
      " [  3   3   0  19   1   0   1   0   0   0   0   0   2   0   0]\n",
      " [  8   0   2   0  11   0   1   0   1   0   0   0   0   0   0]\n",
      " [ 16   0   1   0   0  15   1   3   1   6   1   0   0   0   0]\n",
      " [ 16   0   1   0   0   0  32   0   0   0   0   0   0   0   0]\n",
      " [  5   0   1   0   0   1   0  48   0   1   2   0   0   0   0]\n",
      " [  3   1   0   0   0   0   1   1  16   0   1   0   0   0   0]\n",
      " [ 12   0   0   0   0   7   1   0   0  19   1   0   0   0   0]\n",
      " [  5   0   1   0   0   1   0   2   0   1  10   1   0   0   0]\n",
      " [  2   0   0   0   1   0   0   0   0   0   0  17   1   9   1]\n",
      " [  2   0   0   2   0   0   0   0   0   0   0   0  30   2   1]\n",
      " [  2   1   5   1   1   0   0   0   0   0   0   6   3 109   1]\n",
      " [  3   0   1   0   0   0   1   0   1   0   0   1   3   1  40]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_lr.best_estimator_.predict(X_test)\n",
    "#predictions = clf_svc.best_estimator_.predict_proba(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "#print (\"logloss: %0.3f \" % multiclass_logloss(y_test,predictions))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "log_entry = pd.DataFrame([[\"LogisticRegression_best_estimator_gcv\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols)\n",
    "log = log.append(log_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec embedding and Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use pretrined glove embeddings to train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"dataset/glove.6B/glove.6B.100d.w2vformat.txt\")\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Assignment_group</th>\n",
       "      <th>Target</th>\n",
       "      <th>cleaned_description</th>\n",
       "      <th>num_wds</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>uniq_wds</th>\n",
       "      <th>token_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GRP_0</td>\n",
       "      <td>L1/L2</td>\n",
       "      <td>login issue verified user detailsemployee mana...</td>\n",
       "      <td>20</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>17</td>\n",
       "      <td>login issue verified user detailsemployee mana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GRP_0</td>\n",
       "      <td>L1/L2</td>\n",
       "      <td>outlook hmjdrvpbkomuaywn teammy meetingsskype ...</td>\n",
       "      <td>12</td>\n",
       "      <td>8.083333</td>\n",
       "      <td>11</td>\n",
       "      <td>outlook hmjdrvpbkomuaywn teammy meetingsskype ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GRP_0</td>\n",
       "      <td>L1/L2</td>\n",
       "      <td>cant log vpn eylqgodmybqkwiami cannot log vpn</td>\n",
       "      <td>7</td>\n",
       "      <td>5.571429</td>\n",
       "      <td>5</td>\n",
       "      <td>cant log vpn eylqgodmybqkwiami cannot log vpn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GRP_0</td>\n",
       "      <td>L1/L2</td>\n",
       "      <td>unable access hrtool page unable access hrtool...</td>\n",
       "      <td>8</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>4</td>\n",
       "      <td>unable access hrtool page unable access hrtool...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GRP_0</td>\n",
       "      <td>L1/L2</td>\n",
       "      <td>skype error skype error</td>\n",
       "      <td>4</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>skype error skype error</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Assignment_group Target                                cleaned_description  \\\n",
       "0            GRP_0  L1/L2  login issue verified user detailsemployee mana...   \n",
       "1            GRP_0  L1/L2  outlook hmjdrvpbkomuaywn teammy meetingsskype ...   \n",
       "2            GRP_0  L1/L2      cant log vpn eylqgodmybqkwiami cannot log vpn   \n",
       "3            GRP_0  L1/L2  unable access hrtool page unable access hrtool...   \n",
       "4            GRP_0  L1/L2                            skype error skype error   \n",
       "\n",
       "   num_wds  avg_word  uniq_wds  \\\n",
       "0       20  6.900000        17   \n",
       "1       12  8.083333        11   \n",
       "2        7  5.571429         5   \n",
       "3        8  5.500000         4   \n",
       "4        4  5.000000         2   \n",
       "\n",
       "                                          token_desc  \n",
       "0  login issue verified user detailsemployee mana...  \n",
       "1  outlook hmjdrvpbkomuaywn teammy meetingsskype ...  \n",
       "2      cant log vpn eylqgodmybqkwiami cannot log vpn  \n",
       "3  unable access hrtool page unable access hrtool...  \n",
       "4                            skype error skype error  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_inc_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tranlated_inc = pd.read_csv(\"dataset/cleaned_data.csv\",encoding='utf-8')\n",
    "# df_tranlated_inc = df_tranlated_inc.drop(['Unnamed: 0','cleaned_description','language'],axis=1)\n",
    "\n",
    "#df_inc_sample = df_tranlated_inc[df_tranlated_inc['Assignment group'].map(df_tranlated_inc['Assignment group'].value_counts()) > 100]\n",
    "#df_inc_sample.rename(columns = {'Assignment group':'Assignment_group'}, inplace = True)\n",
    "\n",
    "\n",
    "# df_inc_sample = df_inc_sample.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# grp0_sample = df_inc_sample[df_inc_sample['Assignment_group']=='GRP_0'].sample(n=1500)\n",
    "# other_rows = df_inc_sample[df_inc_sample['Assignment_group']!='GRP_0']\n",
    "# df_inc_sample = pd.concat([other_rows,grp0_sample]).reset_index(drop=True)\n",
    "\n",
    "train, test = train_test_split(df_inc_sample, test_size=0.2, random_state = 13,stratify=df_inc_sample['Assignment_group'])\n",
    "\n",
    "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['token_desc']), axis=1).values\n",
    "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['token_desc']), axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:cannot compute similarity with no input ['collaborationplatform', 'syncng', 'collaborationplatform', 'syncng']\n",
      "WARNING:root:cannot compute similarity with no input ['zurcksetzen', '09122016received']\n",
      "WARNING:root:cannot compute similarity with no input ['lfter', 'defekt', 'industriekontrollmonitor', 'niptbwdq', 'csenjruz', 'lfter', 'defekt', 'industriekontrollmonitor', 'niptbwdq', 'csenjruz']\n",
      "WARNING:root:cannot compute similarity with no input ['collaborationplatform', 'collaborationplatform']\n",
      "WARNING:root:cannot compute similarity with no input ['tastatur', 'defekt', 'wcupoaty', 'fqnzwphj', 'tastatur', 'defekt', 'wcupoaty', 'fqnzwphj']\n",
      "WARNING:root:cannot compute similarity with no input ['uacyltoe', 'hxgaycze', 'uacyltoe', 'hxgaycze']\n",
      "WARNING:root:cannot compute similarity with no input ['uacyltoe', 'hxgaycze', 'uacyltoe', 'hxgaycze']\n",
      "WARNING:root:cannot compute similarity with no input ['arcving', 'emailsreceived']\n"
     ]
    }
   ],
   "source": [
    "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(wv,test_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5,class_weight='balanced')\n",
    "logreg = logreg.fit(X_train_word_average, train['Assignment_group'])\n",
    "y_pred = logreg.predict(X_test_word_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.46766169154228854\n",
      "f1 score 0.4156525934808569\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.96      0.49      0.65       794\n",
      "      GRP_10       0.20      0.36      0.25        28\n",
      "      GRP_12       0.31      0.52      0.39        50\n",
      "      GRP_13       0.19      0.34      0.25        29\n",
      "      GRP_14       0.17      0.39      0.23        23\n",
      "      GRP_19       0.21      0.50      0.30        44\n",
      "       GRP_2       0.24      0.69      0.36        49\n",
      "      GRP_24       0.53      0.72      0.61        58\n",
      "      GRP_25       0.10      0.22      0.14        23\n",
      "       GRP_3       0.17      0.35      0.23        40\n",
      "      GRP_33       0.14      0.33      0.19        21\n",
      "       GRP_5       0.16      0.77      0.27        31\n",
      "       GRP_6       0.32      0.38      0.35        37\n",
      "       GRP_8       0.88      0.34      0.49       129\n",
      "       GRP_9       0.33      0.08      0.13        51\n",
      "\n",
      "    accuracy                           0.47      1407\n",
      "   macro avg       0.33      0.43      0.32      1407\n",
      "weighted avg       0.71      0.47      0.52      1407\n",
      "\n",
      "[[393  26  34  25  24  62  86  26  27  45  34   0   4   3   5]\n",
      " [  1  10   0   1   0   0   0   0   2   0   0  14   0   0   0]\n",
      " [  1   3  26   0   5   1   5   2   0   4   1   0   0   2   0]\n",
      " [  0   5   1  10   3   0   2   0   2   1   0   2   1   0   2]\n",
      " [  1   0   3   2   9   1   1   1   5   0   0   0   0   0   0]\n",
      " [  5   0   3   0   0  22   2   2   1   6   2   0   1   0   0]\n",
      " [  4   1   1   2   3   1  34   0   1   0   1   0   0   0   1]\n",
      " [  2   2   2   2   0   1   1  42   0   4   2   0   0   0   0]\n",
      " [  2   0   1   2   3   1   4   2   5   2   0   0   0   1   0]\n",
      " [  0   0   2   1   2   9   4   2   2  14   4   0   0   0   0]\n",
      " [  0   0   1   0   1   3   0   3   2   4   7   0   0   0   0]\n",
      " [  1   1   0   0   0   0   0   0   0   0   0  24   5   0   0]\n",
      " [  0   0   1   2   1   1   0   0   0   0   0  18  14   0   0]\n",
      " [  0   1   7   2   2   0   0   0   0   0   0  64   9  44   0]\n",
      " [  0   2   2   3   1   1   0   0   1   0   0  27  10   0   4]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report,f1_score\n",
    "print('accuracy %s' % accuracy_score(y_pred, test.Assignment_group))\n",
    "print('f1 score %s' % f1_score(y_pred, test.Assignment_group,average='weighted'))\n",
    "print(classification_report(test.Assignment_group, y_pred))\n",
    "\n",
    "\n",
    "print(confusion_matrix(test.Assignment_group,y_pred))\n",
    "log_entry = pd.DataFrame([[\"Word2Vec - LogisticRegression\",accuracy_score(y_pred, test.Assignment_group),f1_score(y_pred, test.Assignment_group,average='weighted')]], columns=log_cols)\n",
    "log = log.append(log_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> We see that the performance is very poor than the benchmark model, as the pretrained embedding used is not specific to ITSM data and more related to generic english texts </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "xgb_clf = xgb_clf.fit(X_train_word_average, train['Assignment_group'],sample_weight=w_array)\n",
    "y_pred = xgb_clf.predict(X_test_word_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6609808102345416\n",
      "f1 score 0.6897078287081292\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       GRP_0       0.78      0.92      0.85       794\n",
      "      GRP_10       0.00      0.00      0.00        28\n",
      "      GRP_12       0.49      0.36      0.41        50\n",
      "      GRP_13       0.33      0.24      0.28        29\n",
      "      GRP_14       0.35      0.26      0.30        23\n",
      "      GRP_19       0.24      0.14      0.17        44\n",
      "       GRP_2       0.60      0.51      0.55        49\n",
      "      GRP_24       0.75      0.71      0.73        58\n",
      "      GRP_25       0.50      0.22      0.30        23\n",
      "       GRP_3       0.19      0.10      0.13        40\n",
      "      GRP_33       0.38      0.14      0.21        21\n",
      "       GRP_5       0.16      0.74      0.26        31\n",
      "       GRP_6       0.32      0.30      0.31        37\n",
      "       GRP_8       0.89      0.36      0.52       129\n",
      "       GRP_9       1.00      0.02      0.04        51\n",
      "\n",
      "    accuracy                           0.66      1407\n",
      "   macro avg       0.46      0.33      0.34      1407\n",
      "weighted avg       0.68      0.66      0.63      1407\n",
      "\n",
      "[[733   1   9   3   3  10  13   7   2   9   4   0   0   0   0]\n",
      " [ 12   0   0   2   0   0   0   0   0   0   0  14   0   0   0]\n",
      " [ 18   0  18   0   4   0   0   2   1   2   0   0   0   5   0]\n",
      " [ 18   1   0   7   0   0   1   0   1   0   0   1   0   0   0]\n",
      " [ 12   0   2   1   6   0   1   0   1   0   0   0   0   0   0]\n",
      " [ 30   0   3   0   0   6   1   1   0   3   0   0   0   0   0]\n",
      " [ 23   0   0   1   0   0  25   0   0   0   0   0   0   0   0]\n",
      " [ 14   0   0   0   0   0   0  41   0   2   1   0   0   0   0]\n",
      " [ 16   0   0   0   1   0   0   1   5   0   0   0   0   0   0]\n",
      " [ 28   0   0   0   0   7   1   0   0   4   0   0   0   0   0]\n",
      " [ 10   0   1   0   0   2   0   3   0   1   3   0   0   1   0]\n",
      " [  3   0   0   0   0   0   0   0   0   0   0  23   5   0   0]\n",
      " [  4   0   1   3   0   0   0   0   0   0   0  18  11   0   0]\n",
      " [  6   0   2   1   3   0   0   0   0   0   0  61   9  47   0]\n",
      " [ 10   0   1   3   0   0   0   0   0   0   0  27   9   0   1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report,f1_score\n",
    "print('accuracy %s' % accuracy_score(y_pred, test.Assignment_group))\n",
    "print('f1 score %s' % f1_score(y_pred, test.Assignment_group,average='weighted'))\n",
    "print(classification_report(test.Assignment_group, y_pred))\n",
    "\n",
    "print(confusion_matrix(test.Assignment_group,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2vec and Logistic Regression\n",
    "\n",
    "Let's try using Doc2vec, doc2vec is an extension to the word2vec-approach towards documents. Its intention is encode (whole) docs, consisting of lists of sentences, rather than lists of ungrouped sentences. \n",
    "\n",
    "There are two approaches in Doc2vec:\n",
    "    - Distributed Memory\n",
    "    - Distributed BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the post.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(TaggedDocument(v.split(), tags=[label]))\n",
    "    return labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(df_inc_sample.token_desc, df_inc_sample.Assignment_group, test_size=0.2, random_state = 13,stratify=df_inc_sample['Assignment_group'])\n",
    "X_train = label_sentences(X_train, 'Train')\n",
    "X_test = label_sentences(X_test, 'Test')\n",
    "all_data = X_train + X_test\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "# encoding train labels \n",
    "encoder.fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "\n",
    "class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "class_weights\n",
    "\n",
    "w_array = np.ones(y_train.shape[0], dtype = 'float')\n",
    "for i, val in enumerate(y_train):\n",
    "    w_array[i] = class_weights[val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['unable', 'login', 'collaborationplatform', 'unable', 'login', 'collaborationplatform'], tags=['Train_0']),\n",
       " TaggedDocument(words=['hostname66volume', 'f', 'labeldat1hostname66', '9a625d75', 'server', '85', 'space', 'consumed', 'space', 'available', '648', 'k', 'volume', 'f', 'labeldat1hostname66', '9a625d75', 'server', '85', 'space', 'consumed', 'space', 'available', '648', 'k'], tags=['Train_1'])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed BOW\n",
    "DBOW is the doc2vec model analogous to Skip-gram model in word2vec. The paragraph vectors are obtained by training a neural network on the task of predicting a probability distribution of words in a paragraph given a randomly-sampled word from the paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_doc2vec(corpus):\n",
    "    logging.info('Building Doc2Vec model')\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "    model_dbow.build_vocab(corpus)\n",
    "    return model_dbow\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 7033/7033 [00:00<00:00, 2349170.98it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Training DBOW model for 30 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(40):\n",
    "    model_dbow.train(utils.shuffle([x for x in all_data]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the Final Vector Feature for the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\n",
    "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Logistic Regression Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5,class_weight='balanced')\n",
    "logreg.fit(train_vectors_dbow, y_train)\n",
    "y_pred = logreg.predict(test_vectors_dbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5678749111584932\n",
      "f1 score 0.5209679345597619\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.58      0.72       794\n",
      "           1       0.50      0.61      0.55        28\n",
      "           2       0.33      0.52      0.40        50\n",
      "           3       0.48      0.55      0.52        29\n",
      "           4       0.22      0.48      0.30        23\n",
      "           5       0.17      0.45      0.25        44\n",
      "           6       0.29      0.61      0.40        49\n",
      "           7       0.73      0.79      0.76        58\n",
      "           8       0.28      0.61      0.38        23\n",
      "           9       0.17      0.38      0.24        40\n",
      "          10       0.19      0.52      0.28        21\n",
      "          11       0.38      0.55      0.45        31\n",
      "          12       0.36      0.54      0.43        37\n",
      "          13       0.82      0.54      0.65       129\n",
      "          14       0.38      0.49      0.43        51\n",
      "\n",
      "    accuracy                           0.57      1407\n",
      "   macro avg       0.42      0.55      0.45      1407\n",
      "weighted avg       0.74      0.57      0.61      1407\n",
      "\n",
      "[[461   9  32  10  25  70  64   6  22  56  30   1   3   1   4]\n",
      " [  0  17   0   0   1   0   0   0   2   0   0   0   1   2   5]\n",
      " [  2   0  26   0   5   0   1   4   3   3   2   0   1   2   1]\n",
      " [  0   4   1  16   0   0   1   0   3   0   0   0   3   0   1]\n",
      " [  2   2   3   0  11   3   0   0   1   0   0   0   0   0   1]\n",
      " [  6   0   2   0   0  20   1   1   1   9   4   0   0   0   0]\n",
      " [  8   0   4   2   1   2  30   1   0   0   0   0   0   0   1]\n",
      " [  1   0   2   1   0   2   0  46   0   1   5   0   0   0   0]\n",
      " [  1   0   0   0   1   1   0   3  14   2   1   0   0   0   0]\n",
      " [  1   0   2   0   0  14   3   0   1  15   4   0   0   0   0]\n",
      " [  1   1   1   0   0   3   0   2   1   1  11   0   0   0   0]\n",
      " [  0   0   0   0   1   1   0   0   0   0   0  17   4   6   2]\n",
      " [  0   1   0   1   0   0   1   0   0   0   0   5  20   3   6]\n",
      " [  0   0   5   1   5   0   0   0   1   0   0  14  14  70  19]\n",
      " [  1   0   2   2   0   0   1   0   1   0   0   8  10   1  25]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('f1 score %s' % f1_score(y_pred,y_test,average='weighted'))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "log_entry = pd.DataFrame([[\"Doc2Vec (dbow) - LogisticRegression\",accuracy_score(y_pred, y_test),f1_score(y_pred,y_test,average='weighted')]], columns=log_cols)\n",
    "log = log.append(log_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7484008528784648\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.93      0.89       794\n",
      "           1       0.63      0.43      0.51        28\n",
      "           2       0.47      0.54      0.50        50\n",
      "           3       0.53      0.55      0.54        29\n",
      "           4       0.39      0.39      0.39        23\n",
      "           5       0.47      0.32      0.38        44\n",
      "           6       0.61      0.47      0.53        49\n",
      "           7       0.80      0.81      0.80        58\n",
      "           8       0.50      0.39      0.44        23\n",
      "           9       0.62      0.38      0.47        40\n",
      "          10       0.55      0.29      0.37        21\n",
      "          11       0.27      0.13      0.17        31\n",
      "          12       0.41      0.32      0.36        37\n",
      "          13       0.68      0.79      0.73       129\n",
      "          14       0.42      0.31      0.36        51\n",
      "\n",
      "    accuracy                           0.75      1407\n",
      "   macro avg       0.55      0.47      0.50      1407\n",
      "weighted avg       0.73      0.75      0.73      1407\n",
      "\n",
      "[[741   1  12   1   5   7  10   3   4   3   2   0   2   0   3]\n",
      " [  3  12   1   4   1   0   0   0   0   0   0   0   0   5   2]\n",
      " [ 11   0  27   0   5   0   1   3   1   1   0   0   0   1   0]\n",
      " [  4   3   0  16   0   0   1   0   2   0   0   0   2   0   1]\n",
      " [ 10   1   2   0   9   0   0   0   1   0   0   0   0   0   0]\n",
      " [ 24   0   2   0   0  14   1   0   0   3   0   0   0   0   0]\n",
      " [ 24   0   2   0   0   0  23   0   0   0   0   0   0   0   0]\n",
      " [  6   0   1   0   0   1   0  47   0   1   2   0   0   0   0]\n",
      " [ 10   0   0   0   0   0   1   2   9   0   1   0   0   0   0]\n",
      " [ 15   0   3   0   0   5   1   1   0  15   0   0   0   0   0]\n",
      " [  5   0   1   0   1   3   0   3   1   1   6   0   0   0   0]\n",
      " [  3   0   0   0   0   0   0   0   0   0   0   4   3  16   5]\n",
      " [  2   0   0   6   0   0   0   0   0   0   0   2  12  13   2]\n",
      " [  1   1   6   0   2   0   0   0   0   0   0   4   4 102   9]\n",
      " [  5   1   1   3   0   0   0   0   0   0   0   5   6  14  16]]\n"
     ]
    }
   ],
   "source": [
    "xgb_clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "xgb_clf = xgb_clf.fit(train_vectors_dbow, y_train,sample_weight=w_array)\n",
    "y_pred = xgb_clf.predict(test_vectors_dbow)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "#print('f1 score %s' % f1_score(y_pred,y_test,average='weighted'))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Memory\n",
    "\n",
    "Distributed Memory (DM) acts as a memory that remembers what is missing from the current context  or as the topic of the paragraph. While the word vectors represent the concept of a word, the document vector intends to represent the concept of a document. We again instantiate a Doc2Vec model with a vector size with 300 words and iterating over the training corpus 30 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_dm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=3, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "model_dm.build_vocab([x for x in all_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(40):\n",
    "    model_dm.train(utils.shuffle([x for x in all_data]), total_examples=len(all_data), epochs=1)\n",
    "    model_dm.alpha -= 0.002\n",
    "    model_dm.min_alpha = model_dm.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_vectors_dm = get_vectors(model_dm, len(X_train), 300, 'Train')\n",
    "test_vectors_dm = get_vectors(model_dm, len(X_test), 300, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.4214641080312722\n",
      "f1 score 0.38158314075327643\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.41      0.56       794\n",
      "           1       0.16      0.46      0.23        28\n",
      "           2       0.25      0.44      0.32        50\n",
      "           3       0.40      0.59      0.47        29\n",
      "           4       0.20      0.43      0.27        23\n",
      "           5       0.19      0.30      0.23        44\n",
      "           6       0.25      0.45      0.32        49\n",
      "           7       0.52      0.74      0.61        58\n",
      "           8       0.16      0.26      0.20        23\n",
      "           9       0.24      0.57      0.34        40\n",
      "          10       0.15      0.43      0.22        21\n",
      "          11       0.16      0.42      0.23        31\n",
      "          12       0.16      0.19      0.18        37\n",
      "          13       0.39      0.43      0.41       129\n",
      "          14       0.18      0.27      0.22        51\n",
      "\n",
      "    accuracy                           0.42      1407\n",
      "   macro avg       0.29      0.43      0.32      1407\n",
      "weighted avg       0.62      0.42      0.46      1407\n",
      "\n",
      "[[326  45  40  15  25  46  55  19  16  47  37  22  13  54  34]\n",
      " [  0  13   1   0   1   0   1   1   1   0   0   1   5   1   3]\n",
      " [  2   0  22   1   7   0   2   3   4   4   1   0   0   4   0]\n",
      " [  1   3   0  17   0   0   2   1   0   0   0   0   3   1   1]\n",
      " [  0   0   0   2  10   0   1   2   1   1   1   0   1   3   1]\n",
      " [  8   0   2   0   1  13   1   4   0   9   0   2   1   3   0]\n",
      " [ 11   1   5   1   1   1  22   1   2   1   1   1   0   1   0]\n",
      " [  2   0   2   0   0   0   0  43   3   3   3   0   1   1   0]\n",
      " [  4   1   2   1   0   1   1   4   6   1   1   0   1   0   0]\n",
      " [  2   0   2   0   0   3   1   3   2  23   4   0   0   0   0]\n",
      " [  0   0   3   0   0   3   0   1   2   1   9   2   0   0   0]\n",
      " [  1   1   0   1   1   0   0   0   0   0   0  13   1  10   3]\n",
      " [  2   3   1   3   1   0   1   0   0   1   0  11   7   5   2]\n",
      " [  6  10   7   0   4   0   0   0   0   3   3  17   6  55  18]\n",
      " [  5   6   2   2   0   1   2   0   0   0   0  13   4   2  14]]\n"
     ]
    }
   ],
   "source": [
    "logreg = logreg.fit(train_vectors_dm, y_train)\n",
    "y_pred = logreg.predict(test_vectors_dm)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('f1 score %s' % f1_score(y_pred,y_test,average='weighted'))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "log_entry = pd.DataFrame([[\"Doc2Vec (dm) - LogisticRegression\",accuracy_score(y_pred, y_test),f1_score(y_pred,y_test,average='weighted')]], columns=log_cols)\n",
    "log = log.append(log_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7242359630419332\n",
      "f1 score 0.75293351364001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.95      0.88       794\n",
      "           1       0.60      0.32      0.42        28\n",
      "           2       0.54      0.52      0.53        50\n",
      "           3       0.67      0.55      0.60        29\n",
      "           4       0.32      0.26      0.29        23\n",
      "           5       0.53      0.20      0.30        44\n",
      "           6       0.61      0.45      0.52        49\n",
      "           7       0.81      0.74      0.77        58\n",
      "           8       0.50      0.22      0.30        23\n",
      "           9       0.65      0.33      0.43        40\n",
      "          10       0.50      0.29      0.36        21\n",
      "          11       0.32      0.19      0.24        31\n",
      "          12       0.29      0.16      0.21        37\n",
      "          13       0.59      0.68      0.63       129\n",
      "          14       0.24      0.14      0.18        51\n",
      "\n",
      "    accuracy                           0.72      1407\n",
      "   macro avg       0.53      0.40      0.44      1407\n",
      "weighted avg       0.69      0.72      0.70      1407\n",
      "\n",
      "[[757   0   9   1   5   3   4   3   3   4   1   0   0   2   2]\n",
      " [  6   9   0   4   1   0   1   0   0   0   0   0   1   3   3]\n",
      " [ 17   0  26   0   2   0   2   2   0   0   0   0   0   1   0]\n",
      " [  6   2   0  16   0   0   2   0   1   0   0   0   1   1   0]\n",
      " [ 13   0   2   0   6   0   0   0   1   0   0   0   0   1   0]\n",
      " [ 30   0   1   0   0   9   2   1   0   0   1   0   0   0   0]\n",
      " [ 25   0   1   0   0   0  22   0   0   0   0   0   0   0   1]\n",
      " [ 11   0   1   0   0   1   0  43   0   0   2   0   0   0   0]\n",
      " [ 15   0   0   0   0   0   1   1   5   0   0   0   0   1   0]\n",
      " [ 22   0   0   0   0   3   1   0   0  13   1   0   0   0   0]\n",
      " [  5   0   2   1   0   1   0   3   0   3   6   0   0   0   0]\n",
      " [  3   0   0   0   1   0   0   0   0   0   0   6   3  16   2]\n",
      " [  9   1   0   1   0   0   0   0   0   0   0   3   6  15   2]\n",
      " [  5   2   5   0   4   0   0   0   0   0   0   7   6  88  12]\n",
      " [ 10   1   1   1   0   0   1   0   0   0   1   3   4  22   7]]\n"
     ]
    }
   ],
   "source": [
    "xgb_clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "xgb_clf = xgb_clf.fit(train_vectors_dm, y_train,sample_weight=w_array)\n",
    "y_pred = xgb_clf.predict(test_vectors_dm)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('f1 score %s' % f1_score(y_pred,y_test,average='weighted'))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Pairing\n",
    "We also tried combining a paragraph vector from Distributed Bag of Words (DBOW) and Distributed Memory (DM) together for evaluation to check if it improves performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "model_dm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate two models with feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concat_vectors(model1,model2, corpus_size, vectors_size, vectors_type):\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = np.append(model1.docvecs[prefix],model2.docvecs[prefix])\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_dbow_dm = get_concat_vectors(model_dbow,model_dm, len(X_train), 600, 'Train')\n",
    "test_vecs_dbow_dm = get_concat_vectors(model_dbow,model_dm, len(X_test), 600, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Logistic Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.566453447050462\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.60      0.74       794\n",
      "           1       0.52      0.61      0.56        28\n",
      "           2       0.33      0.54      0.41        50\n",
      "           3       0.41      0.62      0.49        29\n",
      "           4       0.28      0.48      0.35        23\n",
      "           5       0.16      0.41      0.23        44\n",
      "           6       0.25      0.57      0.35        49\n",
      "           7       0.69      0.84      0.76        58\n",
      "           8       0.26      0.43      0.33        23\n",
      "           9       0.19      0.40      0.26        40\n",
      "          10       0.23      0.43      0.30        21\n",
      "          11       0.30      0.45      0.36        31\n",
      "          12       0.28      0.41      0.33        37\n",
      "          13       0.77      0.51      0.61       129\n",
      "          14       0.33      0.43      0.38        51\n",
      "\n",
      "    accuracy                           0.57      1407\n",
      "   macro avg       0.40      0.52      0.43      1407\n",
      "weighted avg       0.72      0.57      0.61      1407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg = logreg.fit(train_vecs_dbow_dm, y_train)\n",
    "y_pred = logreg.predict(test_vecs_dbow_dm)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "log_entry = pd.DataFrame([[\"Doc2Vec (dbow + dm) - LogisticRegression\",accuracy_score(y_pred, y_test),f1_score(y_pred,y_test,average='weighted')]], columns=log_cols)\n",
    "log = log.append(log_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7448471926083866\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.97      0.88       794\n",
      "           1       0.64      0.32      0.43        28\n",
      "           2       0.52      0.48      0.50        50\n",
      "           3       0.57      0.45      0.50        29\n",
      "           4       0.41      0.30      0.35        23\n",
      "           5       0.33      0.07      0.11        44\n",
      "           6       0.68      0.43      0.53        49\n",
      "           7       0.85      0.78      0.81        58\n",
      "           8       0.71      0.22      0.33        23\n",
      "           9       0.58      0.28      0.37        40\n",
      "          10       0.80      0.19      0.31        21\n",
      "          11       0.57      0.13      0.21        31\n",
      "          12       0.45      0.27      0.34        37\n",
      "          13       0.62      0.82      0.71       129\n",
      "          14       0.50      0.27      0.35        51\n",
      "\n",
      "    accuracy                           0.74      1407\n",
      "   macro avg       0.60      0.40      0.45      1407\n",
      "weighted avg       0.71      0.74      0.71      1407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "xgb_clf = xgb_clf.fit(train_vecs_dbow_dm, y_train)\n",
    "y_pred = xgb_clf.predict(test_vecs_dbow_dm)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> We dont see much improvement in the performance using these word embeddings compared to the benchmark model using TFID approach <\\b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Classifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc2Vec (dm) - LogisticRegression</th>\n",
       "      <td>0.421464</td>\n",
       "      <td>0.381583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word2Vec - LogisticRegression</th>\n",
       "      <td>0.467662</td>\n",
       "      <td>0.415653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2Vec (dbow) - LogisticRegression</th>\n",
       "      <td>0.567875</td>\n",
       "      <td>0.520968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2Vec (dbow + dm) - LogisticRegression</th>\n",
       "      <td>0.566453</td>\n",
       "      <td>0.521671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Xgboost</th>\n",
       "      <td>0.681592</td>\n",
       "      <td>0.661371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGDClassifier</th>\n",
       "      <td>0.728500</td>\n",
       "      <td>0.708847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultinomialNB</th>\n",
       "      <td>0.658849</td>\n",
       "      <td>0.775546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression_best_estimator_gcv</th>\n",
       "      <td>0.788202</td>\n",
       "      <td>0.784387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD_best_estimator_gcv</th>\n",
       "      <td>0.792466</td>\n",
       "      <td>0.787450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.788202</td>\n",
       "      <td>0.792341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>0.797441</td>\n",
       "      <td>0.797100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC_best_estimator_gcv</th>\n",
       "      <td>0.797441</td>\n",
       "      <td>0.797100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          accuracy  f1_score\n",
       "Classifier                                                  \n",
       "Doc2Vec (dm) - LogisticRegression         0.421464  0.381583\n",
       "Word2Vec - LogisticRegression             0.467662  0.415653\n",
       "Doc2Vec (dbow) - LogisticRegression       0.567875  0.520968\n",
       "Doc2Vec (dbow + dm) - LogisticRegression  0.566453  0.521671\n",
       "Xgboost                                   0.681592  0.661371\n",
       "SGDClassifier                             0.728500  0.708847\n",
       "MultinomialNB                             0.658849  0.775546\n",
       "LogisticRegression_best_estimator_gcv     0.788202  0.784387\n",
       "SGD_best_estimator_gcv                    0.792466  0.787450\n",
       "LogisticRegression                        0.788202  0.792341\n",
       "LinearSVC                                 0.797441  0.797100\n",
       "LinearSVC_best_estimator_gcv              0.797441  0.797100"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.set_index([\"Classifier\"],inplace=True)\n",
    "log.sort_values(by=['f1_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x23d838f2278>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "log.sort_values(by=['f1_score']).plot(kind='barh',figsize=[7,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GRP_8']\n",
      "['GRP_8']\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "sentence = ['The job did not start this morning on time']\n",
    "#get_concat_vectors(model_dbow,model_dm, len(X_train), 1200, 'Train')\n",
    "#mod1 = model_dm.infer_vector(sentence.split())\n",
    "#mod2 =  model_dbow.infer_vector(sentence.split())\n",
    "\n",
    "#ind_vec = np.append(mod1,mod2)\n",
    "\n",
    "\n",
    "#inferred_vector=model_dm.infer_vector(sentence.split())\n",
    "print(encoder.inverse_transform(xgboost.predict(sentence)))\n",
    "print(encoder.inverse_transform(logreg_1.predict(sentence)))\n",
    "\n",
    "#enc.inverse_transform(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['auto_ticket_assignment.pkl']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "### Save the model\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(logreg_1, 'auto_ticket_assignment.pkl', compress=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GRP_0': 0,\n",
       " 'GRP_10': 1,\n",
       " 'GRP_12': 2,\n",
       " 'GRP_13': 3,\n",
       " 'GRP_14': 4,\n",
       " 'GRP_19': 5,\n",
       " 'GRP_2': 6,\n",
       " 'GRP_24': 7,\n",
       " 'GRP_25': 8,\n",
       " 'GRP_3': 9,\n",
       " 'GRP_33': 10,\n",
       " 'GRP_5': 11,\n",
       " 'GRP_6': 12,\n",
       " 'GRP_8': 13,\n",
       " 'GRP_9': 14}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le_name_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "le_name_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GRP_0'], dtype=object)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.externals import joblib\n",
    "model = joblib.load('auto_ticket_assignment.pkl')\n",
    "\n",
    "sentence = 'not able to connect to my system'\n",
    "encoder.inverse_transform(model.predict([sentence]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finals Conclusions fo Approach1\n",
    "\n",
    "- We first analysed the dataset provided to us, undestood the structure of the data provided - number of columns, field , datatypes etc.\n",
    "- We did Exploratory Data Analysis to derive further insights from this data set and we found that\n",
    "    - Data is very much imbalanced, there are around ~45% of the Groups with less than 20 tickets.\n",
    "    - Few of the tickets are in foreign language like German\n",
    "    - The data has lot of noise in it, for eg- few tickets related to account setup are spread across multiple assignment groups.\n",
    "    \n",
    "- We performed the data cleaning and preprocessing\n",
    "    - Translation: A small number of tickets were written in German. Hence, we used the Google translate python api  to convert German to English to generate the input data for the next steps. However, the google translator rest api can only process a limited number of texts on a daily basis, so we translated the text in batches and saved the file for further processing.\n",
    "    - Make text all lowercase so that the algorithm does not treat the same words in different cases as different\n",
    "    - Removing Noise i.e everything that isnt in a standard number or letter i.e Punctuation, Numerical values\n",
    "    - Removing extract spaces\n",
    "    - Removed punctuations\n",
    "    - Removed words containing numbers\n",
    "    - Stopword Removal: Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words\n",
    "    - Lemmatization\n",
    "    - Tokenization: Tokenization is just the term used to describe the process of converting the normal text strings into a list of tokens i.e words that we actually want. Sentence tokenizer can be used to find the list of sentences and Word tokenizer can be used to find the list of words in strings.\n",
    "    \n",
    "\n",
    "- We then ran a basic benchmarck model using the cleaned and preprocessed dataset\n",
    "    - Since the dataset is very imbalanced, We considered a subset of groups for predictions.  In 74 groups, 46% of tickets belong to group 1 and 16 groups just have more than 100 tickets, rest of the Assignment groups have very less ticket counts which might not add much value to the model prediction. If we conducted random sampling towards all the subcategories, then we would face a problem that we might miss all the tickets in some categories. Hence, we considered the groups that have more than 100 tickets. \n",
    "    - We trained the data using below models:\n",
    "        - Multinomial NB\n",
    "        - Linear Support vector Machine\n",
    "        - Logistic regression\n",
    "        - Xgboost\n",
    "        \n",
    "- LinearSVC gives better performance with \n",
    "    - accuracy 0.833642\n",
    "    - f1 score 0.818053\n",
    "\n",
    "<b> Although, it seems like the call is biased towards GRP_0 which has a majority of samples. </b>\n",
    "\n",
    "\n",
    "\n",
    "- Even after downsampling the data we see that the predictions are biased towards GRP_0 which has a majority of samples.\n",
    "- Imbalance causes two problems:\n",
    "    - Training is inefficient as most samples are easy examples that contribute no useful learning signal;\n",
    "    - The easy examples can overwhelm training and lead to degenerate models.\n",
    "    A common solution is to perform some form of hard negative mining that samples hard examples during training or more complex sampling/re weighing schemes.In order to handle the imbalance problem  we used class_weight=balanced hyperparameter while training the model, which tells the model to \"pay more attention\" to samples from an under-represented class.  \n",
    "- Although, the accuracy and f1_score went down. This ensured that the classes were being correctly classified with lesser number of missclassification and good precision/recall scores for all the classes\n",
    "\n",
    "- Next, we also tried using pretrained word embedding, but the only challenge was that we could not find any embeddings trained on ITSM data. We used the glove model with 100d for this, and then used logistic regression and Xgboost to train the model. But, the scores were poorer than the benchmark model.\n",
    " \n",
    "- Then, we also tried vector space modelling using Doc2Vec with DistributedBOW and Distributed Memory approach, though Doc2Vec is a more advanced model in NLP rather than Tf-Idf, but still in our case, it is not giving proper results. We have tried with a linear  & boosting based classifier respectively.\n",
    "  \n",
    "  In our dataset, texts are domain-specific. Furthermore, Doc2Vec model is more suitable for very well written grammatically correct texts. In our case, texts are quite rough in nature.It is also proven in various examples and Data Sc ientists experiments that though Tf-Idf model is inferior as compared to Doc2Vec, but still it gives better result while classifying very domain specific texts.\n",
    " \n",
    " \n",
    " - Linear SVC gave better performance with hyperparameter tuning and this model would be used for classifying the tickets into one of the groups.\n",
    "    - accuracy 0.797441\n",
    "    - f1 score 0.797100\n",
    "\n",
    "The performance can be further improved by collecting more data for tickets and by running deep learning models like RNN and LSTM's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Automated Ticket Assignment - Capstone Project - Divya Kamat.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:nlp] *",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
